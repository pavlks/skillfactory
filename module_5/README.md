# Проект №5 -- Компьютер говорит "Нет"

## Цель проекта
В данном проекте нам необходимо применить наши навыки в предобработке данных и в выборе оптимальной модели обучения и её параметров для предсказания "хороших" и "не очень" клиентов банка и выяснить кому из них можно выдать кредит

Нами в этом модуле были пройдены несколько моделей, которые классифицируют данные, среди них
- DecisionTreeClassifier
- KNeighborsClassifier
- LogisticRegression

Все классификаторы и результат их работы оцениваются метриками (f1_score, accuracy, и т.д.)

## Выводы
Итак, опираясь на проделанную работу я могу сказать, что для повышения показания метрик, нам необходимо уделить большее внимание предварительной обработке данных. Можно сделать следующее:
- попробовать сгенерировать дополнительные признаки, например, с помощью PolynomialFeatures
- можно попробовать найти дополнительную информацию и добавить её в датасет
- можно поменять стратегию заполнения пустых полей в поле `education` и добавить новую категорию `unknown` или же удалить эти данные из набора и посмотреть как изменится результат
- возможно образование коррелирует с другим признаком и подобрать новые значения с этой стороны
В целом, есть куда потратить время, ведь не зря на предобработку уходит до 80% всего времени. **В нашей команде из 2х человек работой с данными занимался Андрей Фёдоров**, я же взялся за то, чтобы попробовать результативность каждой модели.

Говоря о гиперпараметрах, мы видим, что значения показателей по умолчанию - это и есть тот вариант, когда мы достигаем лучшего результата в нашей модели.

В своём проекте я постарался использовать и применить все 3 модели, что мы прошли в этом модуле и посмотреть на их результаты. Конечно, еще придется набраться опыта и начать понимать, когда одна модель более предпочтительна, чем другая. Но на данном этапе я вижу, что Логистическая Регрессия, К-соседей выдают приблизительно одинаковый результат в районе 0.8. Хотя я до конца не уверен, можно ли сравнивать между собой различные метрики (accuracy, f1_score и т.д.) А Деревья не очень подходят под эти данные, потому что очень большая разница у целевого показателя между кол-вом 0 и 1.
